{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangramkesharidash/Google-Colab-files/blob/main/TradeVision_YouTube_ConCall_Sentiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgzuIuWPmGIO",
        "outputId": "c3a6b561-8fb1-4706-fd2f-b09e212df57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Video ID: XEV2Owi-7B4\n",
            "Removing old data: /content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES/XEV2Owi-7B4\n",
            "Downloading fresh audio...\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=XEV2Owi-7B4\n",
            "[youtube] XEV2Owi-7B4: Downloading webpage\n",
            "[youtube] XEV2Owi-7B4: Downloading android sdkless player API JSON\n",
            "[youtube] XEV2Owi-7B4: Downloading tv client config\n",
            "[youtube] XEV2Owi-7B4: Downloading tv player API JSON\n",
            "[youtube] XEV2Owi-7B4: Downloading web safari player API JSON\n",
            "[youtube] XEV2Owi-7B4: Downloading player c6d7bdc9-main\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: [youtube] Falling back to generic n function search\n",
            "         player = https://www.youtube.com/s/player/c6d7bdc9/player_ias.vflset/en_US/base.js\n",
            "WARNING: [youtube] XEV2Owi-7B4: nsig extraction failed: Some formats may be missing\n",
            "         n = i9jePnzzQFuBjAoN ; player = https://www.youtube.com/s/player/c6d7bdc9/player_ias.vflset/en_US/base.js\n",
            "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
            "WARNING: [youtube] XEV2Owi-7B4: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "WARNING: [youtube] XEV2Owi-7B4: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] XEV2Owi-7B4: Downloading m3u8 information\n",
            "[info] XEV2Owi-7B4: Downloading 1 format(s): 251\n",
            "[download] /content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES/Happiest Minds Technologies  Q2FY26 Earnings Concall.webm has already been downloaded\n",
            "[download] 100% of   49.29MiB\n",
            "Downloaded: Happiest Minds Technologies  Q2FY26 Earnings Concall.webm\n",
            "Moved to: /content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES/XEV2Owi-7B4/Happiest_Minds_Technologies__Q2FY26_Earnings_Concall.webm\n",
            "MP3: /content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES/XEV2Owi-7B4/audio.mp3\n",
            "Transcribing seg000.mp3...\n",
            "Transcribing seg001.mp3...\n",
            "Transcribing seg002.mp3...\n",
            "Transcribing seg003.mp3...\n",
            "Transcribing seg004.mp3...\n",
            "Transcribing seg005.mp3...\n",
            "Transcribing seg006.mp3...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------\n",
        "# 1. Install & Import\n",
        "# -------------------------------------------------\n",
        "!pip install -q yt-dlp openai-whisper ffmpeg-python\n",
        "\n",
        "import yt_dlp, os, re, shutil, subprocess\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. SETTINGS\n",
        "# -------------------------------------------------\n",
        "base_directory = \"/content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES\"\n",
        "os.makedirs(base_directory, exist_ok=True)\n",
        "\n",
        "youtube_url = \"https://www.youtube.com/watch?v=XEV2Owi-7B4\"\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Extract Video ID\n",
        "# -------------------------------------------------\n",
        "video_id = re.search(r\"v=([a-zA-Z0-9_-]{11})\", youtube_url).group(1)\n",
        "print(f\"Video ID: {video_id}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. CREATE FRESH FOLDER\n",
        "# -------------------------------------------------\n",
        "video_dir = os.path.join(base_directory, video_id)\n",
        "if os.path.exists(video_dir):\n",
        "    print(f\"Removing old data: {video_dir}\")\n",
        "    shutil.rmtree(video_dir)\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. DOWNLOAD FRESH AUDIO (FORCE NO CACHE)\n",
        "# -------------------------------------------------\n",
        "def download_audio(url, path):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': os.path.join(path, '%(title)s.%(ext)s'),\n",
        "        'quiet': False,\n",
        "        'no_warnings': False,\n",
        "        'ignoreerrors': False,\n",
        "        'retries': 10,\n",
        "        'fragment_retries': 10,\n",
        "        'extractor_retries': 10,\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(url, download=True)\n",
        "        return info['title'], info['ext']\n",
        "\n",
        "print(\"Downloading fresh audio...\")\n",
        "title, ext = download_audio(youtube_url, base_directory)\n",
        "print(f\"Downloaded: {title}.{ext}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. Move to video folder\n",
        "# -------------------------------------------------\n",
        "downloaded_file = f\"{title}.{ext}\"\n",
        "old_path = os.path.join(base_directory, downloaded_file)\n",
        "new_path = os.path.join(video_dir, downloaded_file.replace(\" \", \"_\"))\n",
        "shutil.move(old_path, new_path)\n",
        "print(f\"Moved to: {new_path}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 7. Convert to MP3\n",
        "# -------------------------------------------------\n",
        "def to_mp3(inp, out):\n",
        "    subprocess.run([\n",
        "        'ffmpeg', '-i', inp, '-q:a', '0', '-map', 'a', out, '-y'\n",
        "    ], check=True, stdout=subprocess.DEVNULL)\n",
        "\n",
        "audio_mp3 = os.path.join(video_dir, \"audio.mp3\")\n",
        "to_mp3(new_path, audio_mp3)\n",
        "print(f\"MP3: {audio_mp3}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 8. Split & Transcribe\n",
        "# -------------------------------------------------\n",
        "def split_audio(mp3, sec=600, prefix=\"seg\"):\n",
        "    pattern = os.path.join(video_dir, f\"{prefix}%03d.mp3\")\n",
        "    subprocess.run([\n",
        "        'ffmpeg', '-i', mp3, '-f', 'segment', '-segment_time', str(sec),\n",
        "        '-c', 'copy', pattern, '-y'\n",
        "    ], check=True, stdout=subprocess.DEVNULL)\n",
        "\n",
        "split_audio(audio_mp3)\n",
        "\n",
        "def transcribe(prefix=\"seg\"):\n",
        "    i = 0\n",
        "    while True:\n",
        "        f = os.path.join(video_dir, f\"{prefix}{i:03d}.mp3\")\n",
        "        if not os.path.exists(f): break\n",
        "        print(f\"Transcribing {os.path.basename(f)}...\")\n",
        "        subprocess.run([\n",
        "            'whisper', f, '--model', 'base', '--language', 'en',\n",
        "            '--output_dir', video_dir, '--output_format', 'txt'\n",
        "        ], check=True)\n",
        "        i += 1\n",
        "    print(\"Done!\")\n",
        "\n",
        "transcribe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "video_dir = \"/content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES/XEV2Owi-7B4\"\n",
        "\n",
        "if not os.path.exists(video_dir):\n",
        "    print(\"FOLDER NOT FOUND!\")\n",
        "    print(\"Run the FULL pipeline again (download + transcribe) WITHOUT deleting the folder.\")\n",
        "else:\n",
        "    print(f\"Folder exists: {video_dir}\")\n",
        "    print(\"Files inside:\")\n",
        "    !ls -la \"{video_dir}\" | head -20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQAmOGIYTsQP",
        "outputId": "5ded0ed4-fb60-4633-c077-aab20ccd6b81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLDER NOT FOUND!\n",
            "Run the FULL pipeline again (download + transcribe) WITHOUT deleting the folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# LIST ALL FILES & FOLDERS UNDER YOUTUBE/FILES\n",
        "# =============================================\n",
        "import os\n",
        "from datetime import datetime\n",
        "import os, re, shutil, subprocess\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = \"/content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES\"\n",
        "\n",
        "print(f\"Scanning: {base_path}\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'TYPE':<6} {'SIZE (MB)':>10} {'MODIFIED':<20} {'NAME'}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "total_dirs = 0\n",
        "total_files = 0\n",
        "total_size = 0\n",
        "\n",
        "# Walk through all subdirectories\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    # Count directories\n",
        "    total_dirs += len(dirs)\n",
        "\n",
        "    # Print each file\n",
        "    for f in files:\n",
        "        file_path = os.path.join(root, f)\n",
        "        try:\n",
        "            stat = os.stat(file_path)\n",
        "            size_mb = stat.st_size / (1024 * 1024)  # MB\n",
        "            mtime = datetime.fromtimestamp(stat.st_mtime).strftime(\"%b %d %I:%M %p\")\n",
        "            rel_path = os.path.relpath(file_path, base_path)\n",
        "            file_type = \"FILE\" if os.path.isfile(file_path) else \"DIR \"\n",
        "\n",
        "            print(f\"{file_type:<6} {size_mb:9.2f}  {mtime:<20} {rel_path}\")\n",
        "            total_files += 1\n",
        "            total_size += stat.st_size\n",
        "        except:\n",
        "            print(f\"{'ERR ':<6} {'-':>10}  {'-':<20} {rel_path}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"=\" * 80)\n",
        "print(f\"TOTAL DIRECTORIES : {total_dirs}\")\n",
        "print(f\"TOTAL FILES       : {total_files}\")\n",
        "print(f\"TOTAL SIZE        : {total_size / (1024*1024):.2f} MB\")\n",
        "print(f\"LOCATION          : {base_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8PAFe2vTsUp",
        "outputId": "f52ab405-17cd-4bcd-83d5-128af669809e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Scanning: /content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES\n",
            "\n",
            "================================================================================\n",
            "TYPE    SIZE (MB) MODIFIED             NAME\n",
            "================================================================================\n",
            "FILE        9.25  Dec 15 01:25 PM      audio.mp3\n",
            "FILE       19.30  Nov 01 06:50 PM      What is MCP？ Integrate AI Agents with Databases & APIs.mp4\n",
            "FILE        5.17  Nov 01 07:09 PM      What_is_MCP_Integrate_AI_Agents_with_Databases__APIs.mp3\n",
            "FILE       40.11  Nov 01 07:16 PM      TTK Prestige Q2FY26 Earnings Concall.webm\n",
            "FILE        7.89  Nov 01 07:16 PM      CaeFzbJynWY/Best_Stocks_To_Buy_Now____YesBank_Ready_To_Blast_____Biggest_Swing_Trade_Opportunity_in_year__24-25.webm\n",
            "FILE       10.79  Nov 01 07:16 PM      CaeFzbJynWY/audio.mp3\n",
            "FILE       10.79  Nov 01 07:16 PM      CaeFzbJynWY/seg000.mp3\n",
            "FILE        0.00  Nov 01 07:18 PM      CaeFzbJynWY/seg000.txt\n",
            "FILE       49.29  Nov 01 07:28 PM      XEV2Owi-7B4/Happiest_Minds_Technologies__Q2FY26_Earnings_Concall.webm\n",
            "FILE       42.53  Nov 01 07:29 PM      XEV2Owi-7B4/audio.mp3\n",
            "FILE        6.80  Nov 01 07:29 PM      XEV2Owi-7B4/seg000.mp3\n",
            "FILE        6.56  Nov 01 07:29 PM      XEV2Owi-7B4/seg001.mp3\n",
            "FILE        6.69  Nov 01 07:29 PM      XEV2Owi-7B4/seg002.mp3\n",
            "FILE        6.79  Nov 01 07:29 PM      XEV2Owi-7B4/seg003.mp3\n",
            "FILE        6.69  Nov 01 07:29 PM      XEV2Owi-7B4/seg004.mp3\n",
            "FILE        6.64  Nov 01 07:29 PM      XEV2Owi-7B4/seg005.mp3\n",
            "FILE        2.36  Nov 01 07:29 PM      XEV2Owi-7B4/seg006.mp3\n",
            "FILE        0.01  Nov 01 07:42 PM      XEV2Owi-7B4/seg000.txt\n",
            "FILE        0.01  Nov 01 07:59 PM      XEV2Owi-7B4/seg001.txt\n",
            "FILE        0.01  Nov 01 08:15 PM      XEV2Owi-7B4/seg002.txt\n",
            "FILE        0.01  Nov 01 08:26 PM      XEV2Owi-7B4/seg003.txt\n",
            "FILE        0.01  Nov 01 08:38 PM      XEV2Owi-7B4/seg004.txt\n",
            "FILE        0.01  Nov 01 08:54 PM      XEV2Owi-7B4/seg005.txt\n",
            "FILE        0.00  Nov 01 09:01 PM      XEV2Owi-7B4/seg006.txt\n",
            "================================================================================\n",
            "TOTAL DIRECTORIES : 2\n",
            "TOTAL FILES       : 24\n",
            "TOTAL SIZE        : 237.70 MB\n",
            "LOCATION          : /content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# COMBINE ALL TRANSCRIPTION SEGMENTS\n",
        "# =============================================\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "video_id = \"XEV2Owi-7B4\"\n",
        "base_dir = \"/content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES\"\n",
        "video_dir = os.path.join(base_dir, video_id)\n",
        "\n",
        "# Output file\n",
        "full_transcript_path = os.path.join(video_dir, \"FULL_HAPPIEST_MINDS_Q2FY26_TRANSCRIPT.txt\")\n",
        "\n",
        "print(\"Combining all .txt segments into one file...\\n\")\n",
        "\n",
        "# --- COMBINE LOGIC ---\n",
        "with open(full_transcript_path, \"w\", encoding=\"utf-8\") as full_file:\n",
        "    full_file.write(\"HAPPIEST MINDS TECHNOLOGIES\\n\")\n",
        "    full_file.write(\"Q2 FY26 EARNINGS CONFERENCE CALL - FULL TRANSCRIPT\\n\")\n",
        "    full_file.write(\"=\" * 85 + \"\\n\\n\")\n",
        "\n",
        "    segment_count = 0\n",
        "    total_chars = 0\n",
        "\n",
        "    for i in range(100):  # Support up to 100 segments\n",
        "        txt_file = os.path.join(video_dir, f\"seg{i:03d}.txt\")\n",
        "        if not os.path.exists(txt_file):\n",
        "            break\n",
        "\n",
        "        with open(txt_file, \"r\", encoding=\"utf-8\") as seg:\n",
        "            content = seg.read().strip()\n",
        "            if not content:\n",
        "                continue\n",
        "\n",
        "            start_min = i * 10\n",
        "            end_min = (i + 1) * 10\n",
        "            full_file.write(f\"[{start_min:02d}:00 - {end_min:02d}:00]  SEGMENT {i:03d}\\n\")\n",
        "            full_file.write(\"-\" * 60 + \"\\n\")\n",
        "            full_file.write(content + \"\\n\\n\")\n",
        "\n",
        "            segment_count += 1\n",
        "            total_chars += len(content)\n",
        "\n",
        "    # --- SUMMARY ---\n",
        "    full_file.write(\"\\n\" + \"=\" * 85 + \"\\n\")\n",
        "    full_file.write(\"TRANSCRIPTION SUMMARY\\n\")\n",
        "    full_file.write(\"=\" * 85 + \"\\n\")\n",
        "    full_file.write(f\"Total Segments: {segment_count}\\n\")\n",
        "    full_file.write(f\"Total Duration: {segment_count * 10} minutes\\n\")\n",
        "    full_file.write(f\"Total Characters: {total_chars:,}\\n\")\n",
        "    full_file.write(f\"Estimated Words: {total_chars // 5:,}\\n\")\n",
        "    full_file.write(f\"Generated on: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %I:%M %p')}\\n\")\n",
        "\n",
        "print(f\"Combined transcript saved!\\n\")\n",
        "print(f\"File: {full_transcript_path}\\n\")\n",
        "print(f\"Segments: {segment_count} | Duration: {segment_count * 10} min | Words: ~{total_chars // 5:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPGuaELedf-R",
        "outputId": "657aff44-7c67-48fe-92fa-17fd7758c88f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combining all .txt segments into one file...\n",
            "\n",
            "Combined transcript saved!\n",
            "\n",
            "File: /content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES/XEV2Owi-7B4/FULL_HAPPIEST_MINDS_Q2FY26_TRANSCRIPT.txt\n",
            "\n",
            "Segments: 7 | Duration: 70 min | Words: ~10,354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# DUAL LLM SUMMARISER – AUTO MODEL NAME IN FILENAME\n",
        "# =============================================\n",
        "!pip install -q google-generativeai transformers torch accelerate\n",
        "\n",
        "import os, textwrap, time, torch\n",
        "import google.generativeai as genai\n",
        "from transformers import pipeline\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. CONFIG\n",
        "# -------------------------------------------------\n",
        "GEMINI_API_KEY = \"AIzaSyBwues9VXjn5ZOG3wu8asYlNW_JV9mKKq0\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "VIDEO_ID = \"XEV2Owi-7B4\"\n",
        "BASE_DIR = \"/content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES\"\n",
        "VIDEO_DIR = os.path.join(BASE_DIR, VIDEO_ID)\n",
        "TRANSCRIPT = os.path.join(VIDEO_DIR, \"FULL_HAPPIEST_MINDS_Q2FY26_TRANSCRIPT.txt\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. AUTO-DETECT GEMINI MODEL\n",
        "# -------------------------------------------------\n",
        "print(\"Detecting available Gemini model...\")\n",
        "models = [m for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "gemini_model_name = None\n",
        "for m in models:\n",
        "    if \"flash\" in m.name.lower() or \"pro\" in m.name.lower():\n",
        "        gemini_model_name = m.name\n",
        "        break\n",
        "if not gemini_model_name and models:\n",
        "    gemini_model_name = models[0].name\n",
        "\n",
        "if gemini_model_name:\n",
        "    print(f\"Using Gemini: {gemini_model_name}\")\n",
        "    gemini_model = genai.GenerativeModel(gemini_model_name)\n",
        "    gemini_clean_name = gemini_model_name.split(\"/\")[-1].replace(\":\", \"_\")  # e.g., gemini-1.5-flash-001\n",
        "else:\n",
        "    print(\"No Gemini model available. Skipping Gemini.\")\n",
        "    gemini_model = None\n",
        "    gemini_clean_name = \"gemini_unavailable\"\n",
        "\n",
        "# Flan model name\n",
        "flan_model_name = \"flan-t5-large\"\n",
        "\n",
        "# Output paths with MODEL NAME\n",
        "GEMINI_OUT = os.path.join(VIDEO_DIR, f\"EXECUTIVE_SUMMARY_{gemini_clean_name}.txt\")\n",
        "FLAN_OUT   = os.path.join(VIDEO_DIR, f\"EXECUTIVE_SUMMARY_{flan_model_name}.txt\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. LOAD & SPLIT\n",
        "# -------------------------------------------------\n",
        "with open(TRANSCRIPT, \"r\", encoding=\"utf-8\") as f:\n",
        "    full_text = f.read()\n",
        "print(f\"\\nTranscript: {len(full_text):,} chars\")\n",
        "\n",
        "CHUNK_SIZE = 4000\n",
        "chunks = textwrap.wrap(full_text, CHUNK_SIZE)\n",
        "print(f\"Split: {len(chunks)} chunks\\n\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. GEMINI SUMMARISATION\n",
        "# -------------------------------------------------\n",
        "if gemini_model:\n",
        "    print(f\"GEMINI ({gemini_clean_name}) summarising...\")\n",
        "    gemini_sums = []\n",
        "    for i, c in enumerate(chunks, 1):\n",
        "        print(f\"  Chunk {i}/{len(chunks)}\")\n",
        "        prompt = f\"Summarise in 3-5 bullets. Keep revenue, EBITDA, PAT, growth, guidance:\\n\\n{c}\"\n",
        "        try:\n",
        "            resp = gemini_model.generate_content(prompt)\n",
        "            gemini_sums.append(resp.text.strip())\n",
        "        except Exception as e:\n",
        "            gemini_sums.append(f\"[ERROR: {e}]\")\n",
        "        time.sleep(1.5)\n",
        "\n",
        "    final_prompt = \"Combine into ONE executive summary (max 300 words):\\n\\n\" + \"\\n\\n\".join(gemini_sums)\n",
        "    print(\"Final Gemini summary...\")\n",
        "    try:\n",
        "        final_gemini = gemini_model.generate_content(final_prompt).text.strip()\n",
        "    except Exception as e:\n",
        "        final_gemini = f\"[FINAL ERROR: {e}]\"\n",
        "else:\n",
        "    final_gemini = \"[GEMINI UNAVAILABLE]\"\n",
        "\n",
        "# Save Gemini\n",
        "with open(GEMINI_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"HAPPIEST MINDS Q2 FY26 – {gemini_clean_name.upper()}\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\" + final_gemini)\n",
        "    f.write(f\"\\n\\nGenerated: {time.strftime('%Y-%m-%d %H:%M')} UTC\")\n",
        "print(f\"GEMINI SAVED: {GEMINI_OUT}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. FLAN-T5 SUMMARISATION\n",
        "# -------------------------------------------------\n",
        "print(f\"\\nFLAN-T5 ({flan_model_name}) summarising...\")\n",
        "flan_summarizer = pipeline(\"summarization\", model=\"google/flan-t5-large\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "flan_sums = []\n",
        "for i, c in enumerate(chunks, 1):\n",
        "    print(f\"  Chunk {i}/{len(chunks)}\")\n",
        "    try:\n",
        "        out = flan_summarizer(c[:1000], max_length=150, min_length=50, do_sample=False)[0][\"summary_text\"]\n",
        "        flan_sums.append(out)\n",
        "    except Exception as e:\n",
        "        flan_sums.append(f\"[ERROR: {e}]\")\n",
        "    time.sleep(0.3)\n",
        "\n",
        "final_flan = \"\\n\\n\".join(flan_sums)\n",
        "\n",
        "# Save Flan\n",
        "with open(FLAN_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"HAPPIEST MINDS Q2 FY26 – {flan_model_name.upper()}\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\" + final_flan)\n",
        "    f.write(f\"\\n\\nGenerated: {time.strftime('%Y-%m-%d %H:%M')} UTC\")\n",
        "print(f\"FLAN SAVED: {FLAN_OUT}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. PREVIEW\n",
        "# -------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(f\"GEMINI ({gemini_clean_name}) PREVIEW:\")\n",
        "print(\"=\"*90)\n",
        "print(final_gemini[:1200] + (\"...\" if len(final_gemini) > 1200 else \"\"))\n",
        "\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(f\"FLAN ({flan_model_name}) PREVIEW:\")\n",
        "print(\"=\"*90)\n",
        "print(final_flan[:1200] + (\"...\" if len(final_flan) > 1200 else \"\"))\n",
        "\n",
        "print(f\"\\nDONE! Two model-named files saved in:\\n{VIDEO_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ao6k9OWuzO",
        "outputId": "a1c1ebc0-6788-4af0-ef46-72e86c1d32d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detecting available Gemini model...\n",
            "Using Gemini: models/gemini-2.5-pro-preview-03-25\n",
            "\n",
            "Transcript: 52,906 chars\n",
            "Split: 14 chunks\n",
            "\n",
            "GEMINI (gemini-2.5-pro-preview-03-25) summarising...\n",
            "  Chunk 1/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 658.75ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 2/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 557.77ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 3/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 558.08ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 4/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 481.60ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 5/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 583.63ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 6/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1088.51ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 7/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 734.57ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 8/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 507.44ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 9/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 481.60ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 10/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 532.19ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 11/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 507.15ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 12/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 582.32ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 13/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 506.95ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 14/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 506.57ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Gemini summary...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 558.16ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GEMINI SAVED: /content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES/XEV2Owi-7B4/EXECUTIVE_SUMMARY_gemini-2.5-pro-preview-03-25.txt\n",
            "\n",
            "FLAN-T5 (flan-t5-large) summarising...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 1/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 2/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 3/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 4/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 5/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Chunk 6/14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 7/14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 8/14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 9/14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 10/14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 11/14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 12/14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 13/14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 14/14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLAN SAVED: /content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES/XEV2Owi-7B4/EXECUTIVE_SUMMARY_flan-t5-large.txt\n",
            "\n",
            "==========================================================================================\n",
            "GEMINI (gemini-2.5-pro-preview-03-25) PREVIEW:\n",
            "==========================================================================================\n",
            "[FINAL ERROR: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\n",
            "Please retry in 3.417697761s.]\n",
            "\n",
            "==========================================================================================\n",
            "FLAN (flan-t5-large) PREVIEW:\n",
            "==========================================================================================\n",
            "Anand Rati institutional equities, I would like to begin by thanking you for joining us today for the future of site 26 learning forum at the Estimates Technology Limited. I would like to begin by thanking you for joining us today for the future of site 26 learning forum at the Estimates Technology Limited. I would like to begin by thanking you for joining us today for the future of site 26 learning forum at the Estimates Technology Limited. I would like to begin by thanking you for joining us today for the future of site 26 learning forum at the Estimates Technology Limited. I would like to begin by thanking you for joining us today for the future of site 26 learning forum at the Estimates Technology Limited. I would like to begin by thanking you for joining us today for the future of site 26 learning forum at the Estimates Technology Limited. I would like to begin by thanking you for joining us today for the future of site 26 learning forum at the Estimates Technology Limited. I would like to begin by thanking you for joining us today for the future of site 26 learning forum at the Estimates Technology Limited. I would like to begin by thanking you for joining us today for\n",
            "\n",
            "AI-dr...\n",
            "\n",
            "DONE! Two model-named files saved in:\n",
            "/content/drive/My Drive/DREAM-PROJECT/YOUTUBE/FILES/XEV2Owi-7B4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORlvVJ0w/o85SNDgVN81qz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}